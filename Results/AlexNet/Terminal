C:\Users\stong\Anaconda3\python.exe C:/Users/stong/PycharmProjects/Project_2020_Introvert/main.py

1: LeNet
2: AlexNet
3: VGG-16
4: LeNet_improved

Enter 1-4 to select a model:
2

----Dataset Ratios----
Total dataset size: 70000
Train Set: 68.57%
Validation Set: 17.14%
Test Set: 14.29%

torch.Size([150, 1, 227, 227])
tensor([0, 5, 3, 0, 9, 9, 7, 3, 1, 1, 9, 6, 5, 7, 2, 7, 5, 0, 8, 3, 0, 5, 6, 1,
        1, 3, 6, 3, 2, 6, 7, 4, 6, 1, 4, 2, 1, 1, 7, 1, 7, 0, 7, 3, 9, 2, 7, 5,
        4, 1, 7, 9, 8, 6, 5, 6, 7, 2, 6, 2, 1, 9, 3, 7, 5, 0, 0, 9, 4, 9, 3, 1,
        6, 9, 5, 1, 3, 9, 2, 9, 5, 2, 5, 2, 7, 2, 9, 8, 2, 7, 1, 9, 9, 7, 6, 8,
        7, 4, 2, 4, 3, 2, 1, 4, 3, 8, 9, 1, 6, 9, 8, 9, 9, 2, 7, 0, 6, 9, 5, 8,
        6, 3, 0, 4, 7, 1, 5, 1, 4, 4, 9, 5, 0, 8, 8, 9, 5, 8, 9, 9, 4, 9, 1, 3,
        4, 9, 1, 9, 8, 3])
AlexNet(
  (features): Sequential(
    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU()
    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU()
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU()
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)

AlexNet activated!

cuda:0
[Epoch 1, 32 / 320 Mini Batches] loss: 2.300
[Epoch 1, 64 / 320 Mini Batches] loss: 2.270
[Epoch 1, 96 / 320 Mini Batches] loss: 1.984
[Epoch 1, 128 / 320 Mini Batches] loss: 1.662
[Epoch 1, 160 / 320 Mini Batches] loss: 1.439
[Epoch 1, 192 / 320 Mini Batches] loss: 1.268
[Epoch 1, 224 / 320 Mini Batches] loss: 1.137
[Epoch 1, 256 / 320 Mini Batches] loss: 1.034
[Epoch 1, 288 / 320 Mini Batches] loss: 0.949
[Epoch 1, 320 / 320 Mini Batches] loss: 0.879
[[4473    9   55    6    4   41   31    4   77    3]
 [   0 5272   66   13    5    8    4    8   19    7]
 [  11   11 4582   40   38    7    6   34   40   13]
 [  10    9  217 4374    3  109    5   45   79   41]
 [   3    9   46    3 4272    3   51    7   39  175]
 [   7    5   26   78    9 4063   28    0  111   28]
 [  51   25   32    0   37   50 4553    0   45    0]
 [   3   11  113   39   22   13    0 4671   15  152]
 [  20   43   61   90   19   96   25    5 4289   61]
 [  32   17   54   61   88   35    1   82   63 4284]]
              precision    recall  f1-score   support

           0      0.970     0.951     0.961      4703
           1      0.974     0.976     0.975      5402
           2      0.872     0.958     0.913      4782
           3      0.930     0.894     0.912      4892
           4      0.950     0.927     0.938      4608
           5      0.918     0.933     0.926      4355
           6      0.968     0.950     0.959      4793
           7      0.962     0.927     0.944      5039
           8      0.898     0.911     0.904      4709
           9      0.899     0.908     0.904      4717

    accuracy                          0.934     48000
   macro avg      0.934     0.934     0.934     48000
weighted avg      0.935     0.934     0.934     48000

[[ 946    2    9    0    0    6    5    2    9    1]
 [   0 1118    7    1    0    0    3    1    5    0]
 [   2    1  996    5    3    2    1    7   14    1]
 [   0    1   38  906    0   22    0    8   26    9]
 [   0    2    5    0  926    0   10    1    5   33]
 [   3    0    3   23    1  829    8    1   19    5]
 [  14    4    8    0    9   13  903    0    7    0]
 [   0    2   36   11    1    2    0  936    5   35]
 [   8    2   10   13    7   14    6    6  894   14]
 [   5    5    9   12   14   10    0   12   14  928]]
              precision    recall  f1-score   support

           0      0.967     0.965     0.966       980
           1      0.983     0.985     0.984      1135
           2      0.888     0.965     0.925      1032
           3      0.933     0.897     0.915      1010
           4      0.964     0.943     0.953       982
           5      0.923     0.929     0.926       892
           6      0.965     0.943     0.954       958
           7      0.961     0.911     0.935      1028
           8      0.896     0.918     0.907       974
           9      0.904     0.920     0.912      1009

    accuracy                          0.938     10000
   macro avg      0.938     0.938     0.938     10000
weighted avg      0.939     0.938     0.938     10000

Epoch: 1 of 10, Train Acc: 93.402, Test Acc: 93.820, Loss: 0.879
[Epoch 2, 32 / 320 Mini Batches] loss: 0.216
[Epoch 2, 64 / 320 Mini Batches] loss: 0.205
[Epoch 2, 96 / 320 Mini Batches] loss: 0.196
[Epoch 2, 128 / 320 Mini Batches] loss: 0.190
[Epoch 2, 160 / 320 Mini Batches] loss: 0.186
[Epoch 2, 192 / 320 Mini Batches] loss: 0.183
[Epoch 2, 224 / 320 Mini Batches] loss: 0.179
[Epoch 2, 256 / 320 Mini Batches] loss: 0.173
[Epoch 2, 288 / 320 Mini Batches] loss: 0.170
[Epoch 2, 320 / 320 Mini Batches] loss: 0.168
[[4603    9    5    0    8    5   38    1   33    1]
 [   0 5340   26    5    6    0    2    8   14    1]
 [  18   49 4515   26   28    1   17   61   58    9]
 [  20   14   87 4532    4   41    2   59   82   51]
 [   4    9    2    0 4514    0   30    9    9   31]
 [  18   11    6   41    9 4048   76    4   91   51]
 [  30   18    1    1   17    9 4698    0   19    0]
 [   3   13   38    7   23    0    0 4906   10   39]
 [  18   32   12   18   32   11   45    8 4488   45]
 [  27   19    8   19  161   11    2   85   49 4336]]
              precision    recall  f1-score   support

           0      0.971     0.979     0.975      4703
           1      0.968     0.989     0.978      5402
           2      0.961     0.944     0.952      4782
           3      0.975     0.926     0.950      4892
           4      0.940     0.980     0.959      4608
           5      0.981     0.930     0.955      4355
           6      0.957     0.980     0.968      4793
           7      0.954     0.974     0.964      5039
           8      0.925     0.953     0.939      4709
           9      0.950     0.919     0.934      4717

    accuracy                          0.958     48000
   macro avg      0.958     0.957     0.957     48000
weighted avg      0.958     0.958     0.958     48000

[[ 967    1    1    0    0    0    5    1    5    0]
 [   0 1130    2    1    0    0    1    1    0    0]
 [   9    6  976    3    4    0    2   14   18    0]
 [   2    1    8  958    0    7    0   17   15    2]
 [   0    0    1    0  964    0    8    0    2    7]
 [   5    0    2   18    0  823   13    1   18   12]
 [  11    5    0    0    6    2  930    0    4    0]
 [   0    5   14    2    0    0    0 1003    1    3]
 [  11    1    3    0    7    2   10    5  928    7]
 [   6    8    0    5   28    4    0   17    5  936]]
              precision    recall  f1-score   support

           0      0.956     0.987     0.971       980
           1      0.977     0.996     0.986      1135
           2      0.969     0.946     0.957      1032
           3      0.971     0.949     0.959      1010
           4      0.955     0.982     0.968       982
           5      0.982     0.923     0.951       892
           6      0.960     0.971     0.965       958
           7      0.947     0.976     0.961      1028
           8      0.932     0.953     0.942       974
           9      0.968     0.928     0.947      1009

    accuracy                          0.962     10000
   macro avg      0.962     0.961     0.961     10000
weighted avg      0.962     0.962     0.961     10000

Epoch: 2 of 10, Train Acc: 95.792, Test Acc: 96.150, Loss: 0.168
[Epoch 3, 32 / 320 Mini Batches] loss: 0.129
[Epoch 3, 64 / 320 Mini Batches] loss: 0.129
[Epoch 3, 96 / 320 Mini Batches] loss: 0.123
[Epoch 3, 128 / 320 Mini Batches] loss: 0.123
[Epoch 3, 160 / 320 Mini Batches] loss: 0.121
[Epoch 3, 192 / 320 Mini Batches] loss: 0.119
[Epoch 3, 224 / 320 Mini Batches] loss: 0.117
[Epoch 3, 256 / 320 Mini Batches] loss: 0.114
[Epoch 3, 288 / 320 Mini Batches] loss: 0.114
[Epoch 3, 320 / 320 Mini Batches] loss: 0.111
[[4606    4    8    0    1   20   38    8   12    6]
 [   0 5326   39    5    2    1    1   18   10    0]
 [   3   16 4613   25    5    4    4   85   22    5]
 [   6    6   66 4629    1   81    0   61   12   30]
 [   4   10    7    1 4437    0   23   26    7   93]
 [   4    1    4   19    1 4282   20    6    7   11]
 [  14   10    2    1    4   30 4721    0   11    0]
 [   2    8   22    4    5    0    0 4983    2   13]
 [  10   19   36   41   13   74   36   18 4423   39]
 [  13    8    5   24   38   36    2  103   22 4466]]
              precision    recall  f1-score   support

           0      0.988     0.979     0.984      4703
           1      0.985     0.986     0.985      5402
           2      0.961     0.965     0.963      4782
           3      0.975     0.946     0.960      4892
           4      0.984     0.963     0.974      4608
           5      0.946     0.983     0.964      4355
           6      0.974     0.985     0.980      4793
           7      0.939     0.989     0.963      5039
           8      0.977     0.939     0.958      4709
           9      0.958     0.947     0.952      4717

    accuracy                          0.968     48000
   macro avg      0.969     0.968     0.968     48000
weighted avg      0.969     0.968     0.968     48000

[[ 969    0    2    0    0    0    5    1    3    0]
 [   0 1125    6    0    0    1    1    1    1    0]
 [   2    2  997    4    0    0    1   22    4    0]
 [   1    0    8  964    0   18    0   14    3    2]
 [   0    0    3    0  943    0    8    5    3   20]
 [   3    0    1    2    0  876    4    1    0    5]
 [   6    4    0    0    2    4  938    0    4    0]
 [   0    1   15    1    0    0    0 1007    1    3]
 [  10    1    6    9    4   11    6    8  911    8]
 [   4    4    3    5    8    5    0   19    5  956]]
              precision    recall  f1-score   support

           0      0.974     0.989     0.981       980
           1      0.989     0.991     0.990      1135
           2      0.958     0.966     0.962      1032
           3      0.979     0.954     0.966      1010
           4      0.985     0.960     0.973       982
           5      0.957     0.982     0.970       892
           6      0.974     0.979     0.977       958
           7      0.934     0.980     0.956      1028
           8      0.974     0.935     0.954       974
           9      0.962     0.947     0.955      1009

    accuracy                          0.969     10000
   macro avg      0.969     0.968     0.968     10000
weighted avg      0.969     0.969     0.969     10000

Epoch: 3 of 10, Train Acc: 96.846, Test Acc: 96.860, Loss: 0.111
[Epoch 4, 32 / 320 Mini Batches] loss: 0.118
[Epoch 4, 64 / 320 Mini Batches] loss: 0.111
[Epoch 4, 96 / 320 Mini Batches] loss: 0.104
[Epoch 4, 128 / 320 Mini Batches] loss: 0.100
[Epoch 4, 160 / 320 Mini Batches] loss: 0.097
[Epoch 4, 192 / 320 Mini Batches] loss: 0.097
[Epoch 4, 224 / 320 Mini Batches] loss: 0.096
[Epoch 4, 256 / 320 Mini Batches] loss: 0.097
[Epoch 4, 288 / 320 Mini Batches] loss: 0.095
[Epoch 4, 320 / 320 Mini Batches] loss: 0.094
[[4652    7    6    0    3    3   17    3    6    6]
 [   0 5358   22    0    4    0    0   11    7    0]
 [   7   28 4660    7   13    1    4   37   14   11]
 [   9   12   86 4611    0   46    0   42   26   60]
 [   3    9    4    0 4456    0   12   12    1  111]
 [   7    4    2   15    3 4208   40    4   23   49]
 [  24   15    2    1   13    8 4721    0    9    0]
 [   1   11   28    3   11    0    0 4948    3   34]
 [  25   25   24   14   21   33   32   12 4432   91]
 [  10   11    1    5   26    9    2   33    9 4611]]
              precision    recall  f1-score   support

           0      0.982     0.989     0.985      4703
           1      0.978     0.992     0.985      5402
           2      0.964     0.974     0.969      4782
           3      0.990     0.943     0.966      4892
           4      0.979     0.967     0.973      4608
           5      0.977     0.966     0.971      4355
           6      0.978     0.985     0.981      4793
           7      0.970     0.982     0.976      5039
           8      0.978     0.941     0.959      4709
           9      0.927     0.978     0.952      4717

    accuracy                          0.972     48000
   macro avg      0.972     0.972     0.972     48000
weighted avg      0.972     0.972     0.972     48000

[[ 972    0    1    0    0    0    4    1    1    1]
 [   0 1131    2    0    1    0    0    1    0    0]
 [   4    4 1015    1    1    0    0    5    2    0]
 [   1    0   12  967    0    8    0    9    5    8]
 [   0    1    0    0  948    0    3    3    1   26]
 [   2    1    1    4    0  866    3    1    2   12]
 [   8    4    0    0    6    2  936    0    2    0]
 [   1    5   11    1    0    0    0 1002    1    7]
 [  11    2    5    2    3    2    5    3  929   12]
 [   5    3    0    2    4    0    0    8    1  986]]
              precision    recall  f1-score   support

           0      0.968     0.992     0.980       980
           1      0.983     0.996     0.990      1135
           2      0.969     0.984     0.976      1032
           3      0.990     0.957     0.973      1010
           4      0.984     0.965     0.975       982
           5      0.986     0.971     0.979       892
           6      0.984     0.977     0.981       958
           7      0.970     0.975     0.972      1028
           8      0.984     0.954     0.969       974
           9      0.937     0.977     0.957      1009

    accuracy                          0.975     10000
   macro avg      0.976     0.975     0.975     10000
weighted avg      0.975     0.975     0.975     10000

Epoch: 4 of 10, Train Acc: 97.202, Test Acc: 97.520, Loss: 0.094
[Epoch 5, 32 / 320 Mini Batches] loss: 0.077
[Epoch 5, 64 / 320 Mini Batches] loss: 0.081
[Epoch 5, 96 / 320 Mini Batches] loss: 0.082
[Epoch 5, 128 / 320 Mini Batches] loss: 0.080
[Epoch 5, 160 / 320 Mini Batches] loss: 0.079
[Epoch 5, 192 / 320 Mini Batches] loss: 0.079
[Epoch 5, 224 / 320 Mini Batches] loss: 0.079
[Epoch 5, 256 / 320 Mini Batches] loss: 0.078
[Epoch 5, 288 / 320 Mini Batches] loss: 0.080
[Epoch 5, 320 / 320 Mini Batches] loss: 0.079
[[4666    2    3    1    3    3   14    3    6    2]
 [   0 5340   25    7    5    0    3   14    7    1]
 [   6   17 4634   41    8    0    2   45   27    2]
 [   4    2   25 4787    2   15    0   27   15   15]
 [   2    6    5    0 4520    0    9   13    7   46]
 [   6    2    3   52    4 4206   24    3   36   19]
 [  19   10    4    1   11    2 4730    0   16    0]
 [   1    6   16    8    7    0    0 4979    6   16]
 [   9   10   11   36   11   13    8    7 4570   34]
 [  12    5    4   21   39   10    1   44   26 4555]]
              precision    recall  f1-score   support

           0      0.988     0.992     0.990      4703
           1      0.989     0.989     0.989      5402
           2      0.980     0.969     0.974      4782
           3      0.966     0.979     0.972      4892
           4      0.980     0.981     0.981      4608
           5      0.990     0.966     0.978      4355
           6      0.987     0.987     0.987      4793
           7      0.970     0.988     0.979      5039
           8      0.969     0.970     0.970      4709
           9      0.971     0.966     0.968      4717

    accuracy                          0.979     48000
   macro avg      0.979     0.979     0.979     48000
weighted avg      0.979     0.979     0.979     48000

[[ 972    0    0    0    0    0    4    1    3    0]
 [   0 1128    3    1    1    0    0    1    1    0]
 [   5    3 1009    5    0    0    0    5    5    0]
 [   1    0    1 1002    0    2    0    3    1    0]
 [   0    0    1    1  962    0    1    3    3   11]
 [   1    0    1   14    0  868    3    1    2    2]
 [   8    3    0    0    4    3  935    0    5    0]
 [   0    2    7    2    0    0    0 1014    1    2]
 [   4    1    3    5    1    1    1    3  947    8]
 [   4    1    0   10    8    2    0   10    4  970]]
              precision    recall  f1-score   support

           0      0.977     0.992     0.984       980
           1      0.991     0.994     0.993      1135
           2      0.984     0.978     0.981      1032
           3      0.963     0.992     0.978      1010
           4      0.986     0.980     0.983       982
           5      0.991     0.973     0.982       892
           6      0.990     0.976     0.983       958
           7      0.974     0.986     0.980      1028
           8      0.974     0.972     0.973       974
           9      0.977     0.961     0.969      1009

    accuracy                          0.981     10000
   macro avg      0.981     0.980     0.981     10000
weighted avg      0.981     0.981     0.981     10000

Epoch: 5 of 10, Train Acc: 97.890, Test Acc: 98.070, Loss: 0.079
[Epoch 6, 32 / 320 Mini Batches] loss: 0.070
[Epoch 6, 64 / 320 Mini Batches] loss: 0.073
[Epoch 6, 96 / 320 Mini Batches] loss: 0.072
[Epoch 6, 128 / 320 Mini Batches] loss: 0.073
[Epoch 6, 160 / 320 Mini Batches] loss: 0.071
[Epoch 6, 192 / 320 Mini Batches] loss: 0.071
[Epoch 6, 224 / 320 Mini Batches] loss: 0.072
[Epoch 6, 256 / 320 Mini Batches] loss: 0.071
[Epoch 6, 288 / 320 Mini Batches] loss: 0.069
[Epoch 6, 320 / 320 Mini Batches] loss: 0.069
[[4626    1   15    1    1   10   29    6    7    7]
 [   1 5340   30    3    3    1    5   15    3    1]
 [   1    8 4731    8    2    1    2   22    4    3]
 [   4    3   43 4754    1   38    0   32    6   11]
 [   0    4    6    0 4542    0    9    9    4   34]
 [   3    2    6   11    2 4292   24    2    7    6]
 [   9    6    6    1    4   10 4752    0    5    0]
 [   0    6   27    4    7    1    0 4976    5   13]
 [   9   15   42   26   16   49   42    8 4462   40]
 [   6    3    5   14   50   25    1   39   11 4563]]
              precision    recall  f1-score   support

           0      0.993     0.984     0.988      4703
           1      0.991     0.989     0.990      5402
           2      0.963     0.989     0.976      4782
           3      0.986     0.972     0.979      4892
           4      0.981     0.986     0.984      4608
           5      0.970     0.986     0.977      4355
           6      0.977     0.991     0.984      4793
           7      0.974     0.987     0.981      5039
           8      0.988     0.948     0.968      4709
           9      0.975     0.967     0.971      4717

    accuracy                          0.980     48000
   macro avg      0.980     0.980     0.980     48000
weighted avg      0.980     0.980     0.980     48000

[[ 969    0    1    1    1    2    5    0    0    1]
 [   0 1128    3    0    0    0    2    2    0    0]
 [   0    0 1023    3    0    0    0    5    1    0]
 [   1    0    4  992    0    6    0    7    0    0]
 [   0    0    1    0  964    0    4    3    2    8]
 [   1    0    1    4    0  880    3    1    0    2]
 [   1    3    1    0    2    2  948    0    1    0]
 [   0    1   12    0    0    0    0 1011    1    3]
 [   8    1    7    6    1   10   10    4  920    7]
 [   1    1    3    0    8    6    0    9    0  981]]
              precision    recall  f1-score   support

           0      0.988     0.989     0.988       980
           1      0.995     0.994     0.994      1135
           2      0.969     0.991     0.980      1032
           3      0.986     0.982     0.984      1010
           4      0.988     0.982     0.985       982
           5      0.971     0.987     0.979       892
           6      0.975     0.990     0.982       958
           7      0.970     0.983     0.977      1028
           8      0.995     0.945     0.969       974
           9      0.979     0.972     0.976      1009

    accuracy                          0.982     10000
   macro avg      0.982     0.981     0.981     10000
weighted avg      0.982     0.982     0.982     10000

Epoch: 6 of 10, Train Acc: 97.996, Test Acc: 98.160, Loss: 0.069
[Epoch 7, 32 / 320 Mini Batches] loss: 0.075
[Epoch 7, 64 / 320 Mini Batches] loss: 0.068
[Epoch 7, 96 / 320 Mini Batches] loss: 0.065
[Epoch 7, 128 / 320 Mini Batches] loss: 0.065
[Epoch 7, 160 / 320 Mini Batches] loss: 0.065
[Epoch 7, 192 / 320 Mini Batches] loss: 0.065
[Epoch 7, 224 / 320 Mini Batches] loss: 0.065
[Epoch 7, 256 / 320 Mini Batches] loss: 0.065
[Epoch 7, 288 / 320 Mini Batches] loss: 0.065
[Epoch 7, 320 / 320 Mini Batches] loss: 0.064
[[4675    0    2    1    2    3   13    1    3    3]
 [   1 5370   10    3    2    0    1   12    3    0]
 [   8   22 4674   30    4    0    1   31    9    3]
 [   6    3   14 4817    2   19    0   18    3   10]
 [   3   11    5    0 4546    0   11    7    2   23]
 [   2    1    3   26    3 4283   16    3   12    6]
 [  19   10    4    1    6   11 4736    0    6    0]
 [   1    7   22    9    7    0    0 4979    6    8]
 [  12   16   20   28   13   28   13    5 4549   25]
 [  13    9    3   19   62   16    2   43   13 4537]]
              precision    recall  f1-score   support

           0      0.986     0.994     0.990      4703
           1      0.986     0.994     0.990      5402
           2      0.983     0.977     0.980      4782
           3      0.976     0.985     0.980      4892
           4      0.978     0.987     0.982      4608
           5      0.982     0.983     0.983      4355
           6      0.988     0.988     0.988      4793
           7      0.976     0.988     0.982      5039
           8      0.988     0.966     0.977      4709
           9      0.983     0.962     0.972      4717

    accuracy                          0.983     48000
   macro avg      0.983     0.982     0.983     48000
weighted avg      0.983     0.983     0.983     48000

[[ 972    0    1    0    0    1    2    1    3    0]
 [   0 1129    2    0    1    0    1    2    0    0]
 [   3    2 1020    2    0    0    0    5    0    0]
 [   1    0    0  997    0    6    0    4    2    0]
 [   0    0    2    1  968    0    3    1    0    7]
 [   1    0    1    7    1  875    3    1    0    3]
 [   7    3    0    0    2    2  944    0    0    0]
 [   0    1    7    1    0    0    0 1017    1    1]
 [   7    1    4    6    1    4    2    2  941    6]
 [   4    3    1    4    9    3    0    8    3  974]]
              precision    recall  f1-score   support

           0      0.977     0.992     0.984       980
           1      0.991     0.995     0.993      1135
           2      0.983     0.988     0.986      1032
           3      0.979     0.987     0.983      1010
           4      0.986     0.986     0.986       982
           5      0.982     0.981     0.981       892
           6      0.988     0.985     0.987       958
           7      0.977     0.989     0.983      1028
           8      0.991     0.966     0.978       974
           9      0.983     0.965     0.974      1009

    accuracy                          0.984     10000
   macro avg      0.984     0.983     0.984     10000
weighted avg      0.984     0.984     0.984     10000

Epoch: 7 of 10, Train Acc: 98.263, Test Acc: 98.370, Loss: 0.064
[Epoch 8, 32 / 320 Mini Batches] loss: 0.058
[Epoch 8, 64 / 320 Mini Batches] loss: 0.061
[Epoch 8, 96 / 320 Mini Batches] loss: 0.057
[Epoch 8, 128 / 320 Mini Batches] loss: 0.057
[Epoch 8, 160 / 320 Mini Batches] loss: 0.056
[Epoch 8, 192 / 320 Mini Batches] loss: 0.056
[Epoch 8, 224 / 320 Mini Batches] loss: 0.055
[Epoch 8, 256 / 320 Mini Batches] loss: 0.056
[Epoch 8, 288 / 320 Mini Batches] loss: 0.057
[Epoch 8, 320 / 320 Mini Batches] loss: 0.057
[[4676    1    1    0    2    3   13    0    5    2]
 [   1 5385    4    0    1    0    0    8    3    0]
 [   2   30 4710   14    5    1    1    8    9    2]
 [   4    5   21 4808    0   23    0   10   10   11]
 [   5   10    4    0 4533    0   11    2    5   38]
 [   7    1    1   16    2 4301   15    2    7    3]
 [   9    4    2    1    4    8 4756    0    9    0]
 [   1   15   29   13   10    3    0 4936    7   25]
 [  13   13   19   18    6   42   19    2 4558   19]
 [   9    4    2   13   25   20    1   26   15 4602]]
              precision    recall  f1-score   support

           0      0.989     0.994     0.992      4703
           1      0.985     0.997     0.991      5402
           2      0.983     0.985     0.984      4782
           3      0.985     0.983     0.984      4892
           4      0.988     0.984     0.986      4608
           5      0.977     0.988     0.982      4355
           6      0.988     0.992     0.990      4793
           7      0.988     0.980     0.984      5039
           8      0.985     0.968     0.976      4709
           9      0.979     0.976     0.977      4717

    accuracy                          0.985     48000
   macro avg      0.985     0.985     0.985     48000
weighted avg      0.985     0.985     0.985     48000

[[ 975    0    0    1    0    0    1    0    3    0]
 [   0 1133    1    0    0    0    0    1    0    0]
 [   0    5 1021    1    0    0    0    3    2    0]
 [   1    0    1 1001    0    5    0    2    0    0]
 [   1    1    0    1  961    0    3    1    2   12]
 [   2    0    0    4    0  882    2    1    0    1]
 [   5    2    0    0    1    4  944    0    2    0]
 [   1    6   10    3    0    0    0 1001    1    6]
 [   6    1    3    3    1    3    4    0  946    7]
 [   4    4    0    4    6    4    0    3    2  982]]
              precision    recall  f1-score   support

           0      0.980     0.995     0.987       980
           1      0.984     0.998     0.991      1135
           2      0.986     0.989     0.987      1032
           3      0.983     0.991     0.987      1010
           4      0.992     0.979     0.985       982
           5      0.982     0.989     0.985       892
           6      0.990     0.985     0.987       958
           7      0.989     0.974     0.981      1028
           8      0.987     0.971     0.979       974
           9      0.974     0.973     0.974      1009

    accuracy                          0.985     10000
   macro avg      0.985     0.984     0.985     10000
weighted avg      0.985     0.985     0.985     10000

Epoch: 8 of 10, Train Acc: 98.469, Test Acc: 98.460, Loss: 0.057
[Epoch 9, 32 / 320 Mini Batches] loss: 0.059
[Epoch 9, 64 / 320 Mini Batches] loss: 0.057
[Epoch 9, 96 / 320 Mini Batches] loss: 0.051
[Epoch 9, 128 / 320 Mini Batches] loss: 0.052
[Epoch 9, 160 / 320 Mini Batches] loss: 0.051
[Epoch 9, 192 / 320 Mini Batches] loss: 0.051
[Epoch 9, 224 / 320 Mini Batches] loss: 0.051
[Epoch 9, 256 / 320 Mini Batches] loss: 0.051
[Epoch 9, 288 / 320 Mini Batches] loss: 0.051
[Epoch 9, 320 / 320 Mini Batches] loss: 0.051
[[4661    0    4    2    1    3   15    0   14    3]
 [   1 5346   10    9    5    1    5   12   13    0]
 [   1    5 4686   36    6    0    1   11   35    1]
 [   2    1    9 4817    1   27    0    9   16   10]
 [   2    5    4    0 4544    0    7    1    8   37]
 [   5    0    1   17    1 4281   18    0   25    7]
 [   9    0    3    1    7    6 4755    0   12    0]
 [   3    9   29   24   14    0    0 4913   15   32]
 [   3    2    5    9    4   12    8    1 4650   15]
 [  12    2    1   11   26   14    1   17   17 4616]]
              precision    recall  f1-score   support

           0      0.992     0.991     0.991      4703
           1      0.996     0.990     0.993      5402
           2      0.986     0.980     0.983      4782
           3      0.978     0.985     0.981      4892
           4      0.986     0.986     0.986      4608
           5      0.985     0.983     0.984      4355
           6      0.989     0.992     0.990      4793
           7      0.990     0.975     0.982      5039
           8      0.968     0.987     0.978      4709
           9      0.978     0.979     0.978      4717

    accuracy                          0.985     48000
   macro avg      0.985     0.985     0.985     48000
weighted avg      0.985     0.985     0.985     48000

[[ 973    0    1    0    0    0    1    1    4    0]
 [   0 1127    3    1    2    0    1    0    1    0]
 [   2    1 1014    3    0    0    1    1   10    0]
 [   1    0    0  999    0    6    0    1    3    0]
 [   0    0    0    0  971    0    0    1    2    8]
 [   2    0    0    5    1  877    3    0    2    2]
 [   1    2    0    0    2    5  944    0    4    0]
 [   1    1    9    4    1    0    0 1008    2    2]
 [   2    1    2    0    0    0    1    0  964    4]
 [   1    1    0    3    8    2    0    4    4  986]]
              precision    recall  f1-score   support

           0      0.990     0.993     0.991       980
           1      0.995     0.993     0.994      1135
           2      0.985     0.983     0.984      1032
           3      0.984     0.989     0.987      1010
           4      0.986     0.989     0.987       982
           5      0.985     0.983     0.984       892
           6      0.993     0.985     0.989       958
           7      0.992     0.981     0.986      1028
           8      0.968     0.990     0.979       974
           9      0.984     0.977     0.981      1009

    accuracy                          0.986     10000
   macro avg      0.986     0.986     0.986     10000
weighted avg      0.986     0.986     0.986     10000

Epoch: 9 of 10, Train Acc: 98.477, Test Acc: 98.630, Loss: 0.051
[Epoch 10, 32 / 320 Mini Batches] loss: 0.056
[Epoch 10, 64 / 320 Mini Batches] loss: 0.055
[Epoch 10, 96 / 320 Mini Batches] loss: 0.052
[Epoch 10, 128 / 320 Mini Batches] loss: 0.051
[Epoch 10, 160 / 320 Mini Batches] loss: 0.050
[Epoch 10, 192 / 320 Mini Batches] loss: 0.049
[Epoch 10, 224 / 320 Mini Batches] loss: 0.050
[Epoch 10, 256 / 320 Mini Batches] loss: 0.048
[Epoch 10, 288 / 320 Mini Batches] loss: 0.048
[Epoch 10, 320 / 320 Mini Batches] loss: 0.048
[[4682    0    2    0    1    2    8    1    6    1]
 [   2 5344   15    1    7    0    5   15   11    2]
 [   3    4 4728    3    6    0    2   14   17    5]
 [   8    1   33 4761    0   36    0   14   20   19]
 [   2    2    3    0 4556    0    3    2    5   35]
 [   2    1    2    6    1 4306    6    0   17   14]
 [  25    2    3    0   10   10 4729    0   14    0]
 [   0    6   19    3    9    1    0 4974    8   19]
 [   8    4    6    5    6   17    6    5 4622   30]
 [   8    3    0    1   25    8    1   18   15 4638]]
              precision    recall  f1-score   support

           0      0.988     0.996     0.992      4703
           1      0.996     0.989     0.992      5402
           2      0.983     0.989     0.986      4782
           3      0.996     0.973     0.984      4892
           4      0.986     0.989     0.987      4608
           5      0.983     0.989     0.986      4355
           6      0.993     0.987     0.990      4793
           7      0.986     0.987     0.987      5039
           8      0.976     0.982     0.979      4709
           9      0.974     0.983     0.978      4717

    accuracy                          0.986     48000
   macro avg      0.986     0.986     0.986     48000
weighted avg      0.986     0.986     0.986     48000

[[ 977    0    0    0    0    0    1    1    1    0]
 [   0 1127    3    0    2    0    1    2    0    0]
 [   4    0 1017    0    0    0    0    3    8    0]
 [   2    0    0 1000    0    3    0    3    1    1]
 [   0    0    0    0  966    0    0    0    2   14]
 [   2    0    0    3    0  880    2    1    0    4]
 [   9    2    0    0    2    4  936    0    5    0]
 [   0    1    5    1    0    0    0 1018    1    2]
 [   4    1    3    0    1    0    1    0  957    7]
 [   2    0    1    2    6    1    0    5    2  990]]
              precision    recall  f1-score   support

           0      0.977     0.997     0.987       980
           1      0.996     0.993     0.995      1135
           2      0.988     0.985     0.987      1032
           3      0.994     0.990     0.992      1010
           4      0.989     0.984     0.986       982
           5      0.991     0.987     0.989       892
           6      0.995     0.977     0.986       958
           7      0.985     0.990     0.988      1028
           8      0.980     0.983     0.981       974
           9      0.972     0.981     0.977      1009

    accuracy                          0.987     10000
   macro avg      0.987     0.987     0.987     10000
weighted avg      0.987     0.987     0.987     10000

Epoch: 10 of 10, Train Acc: 98.625, Test Acc: 98.680, Loss: 0.048
29.69 minutes

Process finished with exit code 0
