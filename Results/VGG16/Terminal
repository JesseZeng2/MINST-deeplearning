C:\Users\stong\Anaconda3\python.exe C:/Users/stong/PycharmProjects/Project_2020_Introvert/main.py

1: LeNet
2: AlexNet
3: VGG-16
4: LeNet_improved

Enter 1-4 to select a model:
3

----Dataset Ratios----
Total dataset size: 70000
Train Set: 68.57%
Validation Set: 17.14%
Test Set: 14.29%

torch.Size([4, 1, 224, 224])
tensor([7, 1, 4, 6])
VGG16(
  (conv_block1): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block2): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block3): Sequential(
    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block4): Sequential(
    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block5): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU()
    (2): Linear(in_features=4096, out_features=4096, bias=True)
    (3): ReLU()
    (4): Linear(in_features=4096, out_features=10, bias=True)
  )
)

VGG16 activated!

cuda:0
[Epoch 1, 1200 / 12000 Mini Batches] loss: 2.302
[Epoch 1, 2400 / 12000 Mini Batches] loss: 2.302
[Epoch 1, 3600 / 12000 Mini Batches] loss: 1.753
[Epoch 1, 4800 / 12000 Mini Batches] loss: 1.389
[Epoch 1, 6000 / 12000 Mini Batches] loss: 1.155
[Epoch 1, 7200 / 12000 Mini Batches] loss: 0.995
[Epoch 1, 8400 / 12000 Mini Batches] loss: 0.875
[Epoch 1, 9600 / 12000 Mini Batches] loss: 0.785
[Epoch 1, 10800 / 12000 Mini Batches] loss: 0.712
[Epoch 1, 12000 / 12000 Mini Batches] loss: 0.654
[[4665    0    0    0    3    0   27    9    5    4]
 [   1 5270   25   10    4    0    3   24   12    4]
 [  22   17 4573   16   13    1    7  134   42    3]
 [  11    7   35 4602    3   41    2  105   52   44]
 [   3    6    5    0 4533    0   18   39    7   43]
 [  14    5    3   38    3 4150   49   26   34   22]
 [   9   12    0    0    6    7 4674    0    8    0]
 [   1    5    4    0    5    0    0 5019    0    2]
 [  17   41    9   22   18   12   35   57 4485   27]
 [  19    6    0   10  125   18    1  438    7 4107]]
              precision    recall  f1-score   support

           0      0.980     0.990     0.985      4713
           1      0.982     0.984     0.983      5353
           2      0.983     0.947     0.965      4828
           3      0.980     0.939     0.959      4902
           4      0.962     0.974     0.968      4654
           5      0.981     0.955     0.968      4344
           6      0.971     0.991     0.981      4716
           7      0.858     0.997     0.922      5036
           8      0.964     0.950     0.957      4723
           9      0.965     0.868     0.914      4731

    accuracy                          0.960     48000
   macro avg      0.962     0.960     0.960     48000
weighted avg      0.962     0.960     0.960     48000

[[ 969    0    1    0    0    0    4    2    4    0]
 [   0 1123    3    0    0    0    3    3    3    0]
 [  10    1  978    3    2    0    1   25   12    0]
 [   1    2    5  959    0    9    0   19    8    7]
 [   1    0    2    0  956    0    6    6    2    9]
 [   3    1    0    9    1  854   10    5    4    5]
 [   6    3    0    1    2    2  944    0    0    0]
 [   0    2    4    0    0    0    0 1021    0    1]
 [   7    0    2    4    4    4    6   24  921    2]
 [   5    5    0    4   21    2    1   87    2  882]]
              precision    recall  f1-score   support

           0      0.967     0.989     0.978       980
           1      0.988     0.989     0.989      1135
           2      0.983     0.948     0.965      1032
           3      0.979     0.950     0.964      1010
           4      0.970     0.974     0.972       982
           5      0.980     0.957     0.969       892
           6      0.968     0.985     0.977       958
           7      0.857     0.993     0.920      1028
           8      0.963     0.946     0.954       974
           9      0.974     0.874     0.921      1009

    accuracy                          0.961     10000
   macro avg      0.963     0.960     0.961     10000
weighted avg      0.963     0.961     0.961     10000

Epoch: 1 of 10, Train Acc: 95.996, Test Acc: 96.070, Loss: 0.654
[Epoch 2, 1200 / 12000 Mini Batches] loss: 0.096
[Epoch 2, 2400 / 12000 Mini Batches] loss: 0.094
[Epoch 2, 3600 / 12000 Mini Batches] loss: 0.094
[Epoch 2, 4800 / 12000 Mini Batches] loss: 0.094
[Epoch 2, 6000 / 12000 Mini Batches] loss: 0.090
[Epoch 2, 7200 / 12000 Mini Batches] loss: 0.088
[Epoch 2, 8400 / 12000 Mini Batches] loss: 0.086
[Epoch 2, 9600 / 12000 Mini Batches] loss: 0.085
[Epoch 2, 10800 / 12000 Mini Batches] loss: 0.084
[Epoch 2, 12000 / 12000 Mini Batches] loss: 0.084
[[4701    0    0    1    1    1    6    0    2    1]
 [   3 5307   14    6    1    0    6   11    4    1]
 [   7    4 4759   17    1    0    1   24   14    1]
 [   3    1   14 4810    0   24    0   21   17   12]
 [   2    9    4    0 4604    0   12    3    5   15]
 [   4    2    2   26    2 4274   21    2    6    5]
 [  11    2    3    0    1    6 4691    0    2    0]
 [   1    4   10    0    5    0    0 5005    2    9]
 [  12   25    9   23    5   20   18    8 4597    6]
 [  17    8    1   21   62   11    2   49   14 4546]]
              precision    recall  f1-score   support

           0      0.987     0.997     0.992      4713
           1      0.990     0.991     0.991      5353
           2      0.988     0.986     0.987      4828
           3      0.981     0.981     0.981      4902
           4      0.983     0.989     0.986      4654
           5      0.986     0.984     0.985      4344
           6      0.986     0.995     0.990      4716
           7      0.977     0.994     0.985      5036
           8      0.986     0.973     0.980      4723
           9      0.989     0.961     0.975      4731

    accuracy                          0.985     48000
   macro avg      0.985     0.985     0.985     48000
weighted avg      0.985     0.985     0.985     48000

[[ 974    0    1    0    0    0    1    1    3    0]
 [   0 1126    4    0    0    0    2    2    1    0]
 [   5    0 1014    4    0    0    0    7    2    0]
 [   1    0    0  998    0    3    0    6    2    0]
 [   1    0    3    0  968    0    4    3    1    2]
 [   2    1    0   10    2  870    3    1    1    2]
 [   6    1    0    1    1    2  947    0    0    0]
 [   0    1    5    1    0    0    0 1018    1    2]
 [   6    0    1    7    1    6    2    5  944    2]
 [   6    7    0    6   14    1    0    8    3  964]]
              precision    recall  f1-score   support

           0      0.973     0.994     0.983       980
           1      0.991     0.992     0.992      1135
           2      0.986     0.983     0.984      1032
           3      0.972     0.988     0.980      1010
           4      0.982     0.986     0.984       982
           5      0.986     0.975     0.981       892
           6      0.987     0.989     0.988       958
           7      0.969     0.990     0.979      1028
           8      0.985     0.969     0.977       974
           9      0.992     0.955     0.973      1009

    accuracy                          0.982     10000
   macro avg      0.982     0.982     0.982     10000
weighted avg      0.982     0.982     0.982     10000

Epoch: 2 of 10, Train Acc: 98.529, Test Acc: 98.230, Loss: 0.084
[Epoch 3, 1200 / 12000 Mini Batches] loss: 0.056
[Epoch 3, 2400 / 12000 Mini Batches] loss: 0.054
[Epoch 3, 3600 / 12000 Mini Batches] loss: 0.054
[Epoch 3, 4800 / 12000 Mini Batches] loss: 0.055
[Epoch 3, 6000 / 12000 Mini Batches] loss: 0.054
[Epoch 3, 7200 / 12000 Mini Batches] loss: 0.055
[Epoch 3, 8400 / 12000 Mini Batches] loss: 0.054
[Epoch 3, 9600 / 12000 Mini Batches] loss: 0.054
[Epoch 3, 10800 / 12000 Mini Batches] loss: 0.053
[Epoch 3, 12000 / 12000 Mini Batches] loss: 0.052
[[4692    1    0    0    1    5    6    5    3    0]
 [   0 5330    5    1    1    0    1    8    6    1]
 [  13    5 4708   17    6    1    1   49   26    2]
 [   1    1    4 4815    0   48    0    7   19    7]
 [   0    6    0    0 4624    0    3    1    2   18]
 [   3    2    1    2    2 4310   12    1    7    4]
 [   6    5    1    0    4    6 4689    0    5    0]
 [   0    6    3    3    1    1    0 5011    5    6]
 [   3   13    5    5    6   16    4    2 4663    6]
 [   6    4    0    3   17    9    0   25    7 4660]]
              precision    recall  f1-score   support

           0      0.993     0.996     0.994      4713
           1      0.992     0.996     0.994      5353
           2      0.996     0.975     0.985      4828
           3      0.994     0.982     0.988      4902
           4      0.992     0.994     0.993      4654
           5      0.980     0.992     0.986      4344
           6      0.994     0.994     0.994      4716
           7      0.981     0.995     0.988      5036
           8      0.983     0.987     0.985      4723
           9      0.991     0.985     0.988      4731

    accuracy                          0.990     48000
   macro avg      0.990     0.990     0.990     48000
weighted avg      0.990     0.990     0.990     48000

[[ 972    0    0    0    0    1    2    2    3    0]
 [   0 1132    0    1    0    0    1    0    1    0]
 [   4    1 1002    3    1    0    0   12    9    0]
 [   1    0    0  989    0   13    0    2    2    3]
 [   0    0    1    0  973    0    2    2    2    2]
 [   1    0    0    1    0  887    2    0    1    0]
 [   2    2    0    1    2    5  945    0    1    0]
 [   0    3    2    1    0    0    0 1017    1    4]
 [   3    0    1    2    1    4    0    2  958    3]
 [   0    4    0    1    5    3    0    6    2  988]]
              precision    recall  f1-score   support

           0      0.989     0.992     0.990       980
           1      0.991     0.997     0.994      1135
           2      0.996     0.971     0.983      1032
           3      0.990     0.979     0.985      1010
           4      0.991     0.991     0.991       982
           5      0.972     0.994     0.983       892
           6      0.993     0.986     0.990       958
           7      0.975     0.989     0.982      1028
           8      0.978     0.984     0.981       974
           9      0.988     0.979     0.984      1009

    accuracy                          0.986     10000
   macro avg      0.986     0.986     0.986     10000
weighted avg      0.986     0.986     0.986     10000

Epoch: 3 of 10, Train Acc: 98.963, Test Acc: 98.630, Loss: 0.052
[Epoch 4, 1200 / 12000 Mini Batches] loss: 0.033
[Epoch 4, 2400 / 12000 Mini Batches] loss: 0.036
[Epoch 4, 3600 / 12000 Mini Batches] loss: 0.036
[Epoch 4, 4800 / 12000 Mini Batches] loss: 0.037
[Epoch 4, 6000 / 12000 Mini Batches] loss: 0.037
[Epoch 4, 7200 / 12000 Mini Batches] loss: 0.037
[Epoch 4, 8400 / 12000 Mini Batches] loss: 0.037
[Epoch 4, 9600 / 12000 Mini Batches] loss: 0.036
[Epoch 4, 10800 / 12000 Mini Batches] loss: 0.036
[Epoch 4, 12000 / 12000 Mini Batches] loss: 0.036
[[4710    0    0    0    0    0    3    0    0    0]
 [   2 5284    5    5   13    1    6   16   14    7]
 [   2    1 4810    2    0    0    2    7    2    2]
 [   0    0    3 4888    0    3    0    3    1    4]
 [   1    2    4    1 4595    0   15    2    2   32]
 [   3    0    1   30    0 4297    7    1    3    2]
 [   7    0    0    0    1    5 4701    0    2    0]
 [   0    1    7    3    0    0    0 5013    2   10]
 [   6    1    9    6    0    7    4    4 4677    9]
 [   8    0    1    6    2   11    0   15    3 4685]]
              precision    recall  f1-score   support

           0      0.994     0.999     0.997      4713
           1      0.999     0.987     0.993      5353
           2      0.994     0.996     0.995      4828
           3      0.989     0.997     0.993      4902
           4      0.997     0.987     0.992      4654
           5      0.994     0.989     0.991      4344
           6      0.992     0.997     0.994      4716
           7      0.991     0.995     0.993      5036
           8      0.994     0.990     0.992      4723
           9      0.986     0.990     0.988      4731

    accuracy                          0.993     48000
   macro avg      0.993     0.993     0.993     48000
weighted avg      0.993     0.993     0.993     48000

[[ 975    0    0    0    0    0    2    0    2    1]
 [   1 1118    1    3    0    0    5    1    6    0]
 [   2    1 1018    3    0    0    0    4    3    1]
 [   1    0    0 1005    0    0    0    0    1    3]
 [   1    0    3    0  961    0    4    4    0    9]
 [   2    0    0   17    0  868    1    0    1    3]
 [   4    1    0    1    1    6  944    0    1    0]
 [   0    1    4    1    0    0    0 1016    1    5]
 [   3    0    2    6    0    3    0    2  955    3]
 [   1    0    0    1    2    2    0    5    1  997]]
              precision    recall  f1-score   support

           0      0.985     0.995     0.990       980
           1      0.997     0.985     0.991      1135
           2      0.990     0.986     0.988      1032
           3      0.969     0.995     0.982      1010
           4      0.997     0.979     0.988       982
           5      0.987     0.973     0.980       892
           6      0.987     0.985     0.986       958
           7      0.984     0.988     0.986      1028
           8      0.984     0.980     0.982       974
           9      0.976     0.988     0.982      1009

    accuracy                          0.986     10000
   macro avg      0.986     0.986     0.986     10000
weighted avg      0.986     0.986     0.986     10000

Epoch: 4 of 10, Train Acc: 99.292, Test Acc: 98.570, Loss: 0.036
[Epoch 5, 1200 / 12000 Mini Batches] loss: 0.021
[Epoch 5, 2400 / 12000 Mini Batches] loss: 0.021
[Epoch 5, 3600 / 12000 Mini Batches] loss: 0.022
[Epoch 5, 4800 / 12000 Mini Batches] loss: 0.024
[Epoch 5, 6000 / 12000 Mini Batches] loss: 0.024
[Epoch 5, 7200 / 12000 Mini Batches] loss: 0.024
[Epoch 5, 8400 / 12000 Mini Batches] loss: 0.024
[Epoch 5, 9600 / 12000 Mini Batches] loss: 0.025
[Epoch 5, 10800 / 12000 Mini Batches] loss: 0.025
[Epoch 5, 12000 / 12000 Mini Batches] loss: 0.026
[[4710    0    0    0    0    0    1    0    0    2]
 [   0 5335    0    0    3    0    0    8    7    0]
 [   0    4 4803    5    0    0    0    6   10    0]
 [   0    0    1 4878    0    4    0    3    6   10]
 [   0    3    0    0 4635    0    1    0    3   12]
 [   2    0    1    1    0 4313    2    0   21    4]
 [   5    7    1    0    3    6 4682    0   12    0]
 [   1    4    0    1    0    0    0 4991    9   30]
 [   0    0    1    1    0    3    1    1 4715    1]
 [   0    0    0    0    7    0    0    3    7 4714]]
              precision    recall  f1-score   support

           0      0.998     0.999     0.999      4713
           1      0.997     0.997     0.997      5353
           2      0.999     0.995     0.997      4828
           3      0.998     0.995     0.997      4902
           4      0.997     0.996     0.997      4654
           5      0.997     0.993     0.995      4344
           6      0.999     0.993     0.996      4716
           7      0.996     0.991     0.993      5036
           8      0.984     0.998     0.991      4723
           9      0.988     0.996     0.992      4731

    accuracy                          0.995     48000
   macro avg      0.995     0.995     0.995     48000
weighted avg      0.995     0.995     0.995     48000

[[ 976    0    0    0    0    0    0    0    3    1]
 [   1 1132    0    0    0    0    1    0    1    0]
 [   0    2 1016    3    1    0    0    5    5    0]
 [   0    0    0 1000    0    4    0    1    2    3]
 [   0    0    1    0  975    0    0    2    1    3]
 [   1    0    0    4    0  883    1    0    1    2]
 [   6    3    0    1    3    6  937    0    2    0]
 [   0    2    5    2    0    0    0 1009    1    9]
 [   2    0    0    2    0    1    0    2  966    1]
 [   0    2    0    0    5    3    0    2    5  992]]
              precision    recall  f1-score   support

           0      0.990     0.996     0.993       980
           1      0.992     0.997     0.995      1135
           2      0.994     0.984     0.989      1032
           3      0.988     0.990     0.989      1010
           4      0.991     0.993     0.992       982
           5      0.984     0.990     0.987       892
           6      0.998     0.978     0.988       958
           7      0.988     0.982     0.985      1028
           8      0.979     0.992     0.985       974
           9      0.981     0.983     0.982      1009

    accuracy                          0.989     10000
   macro avg      0.989     0.989     0.989     10000
weighted avg      0.989     0.989     0.989     10000

Epoch: 5 of 10, Train Acc: 99.533, Test Acc: 98.860, Loss: 0.026
[Epoch 6, 1200 / 12000 Mini Batches] loss: 0.016
[Epoch 6, 2400 / 12000 Mini Batches] loss: 0.016
[Epoch 6, 3600 / 12000 Mini Batches] loss: 0.017
[Epoch 6, 4800 / 12000 Mini Batches] loss: 0.017
[Epoch 6, 6000 / 12000 Mini Batches] loss: 0.017
[Epoch 6, 7200 / 12000 Mini Batches] loss: 0.017
[Epoch 6, 8400 / 12000 Mini Batches] loss: 0.018
[Epoch 6, 9600 / 12000 Mini Batches] loss: 0.018
[Epoch 6, 10800 / 12000 Mini Batches] loss: 0.019
[Epoch 6, 12000 / 12000 Mini Batches] loss: 0.019
[[4705    0    0    0    0    0    8    0    0    0]
 [   0 5347    0    0    0    0    0    0    4    2]
 [   0    2 4821    0    0    0    2    1    2    0]
 [   0    0    5 4887    0    2    0    1    4    3]
 [   0    0    0    0 4634    0    8    0    0   12]
 [   0    1    1    5    0 4329    4    0    4    0]
 [   0    2    0    0    0    0 4707    0    7    0]
 [   0    5    7    0    0    0    0 5015    1    8]
 [   0    4    1    0    1    0    1    1 4710    5]
 [   1    0    0    1    3    2    0    8    0 4716]]
              precision    recall  f1-score   support

           0      1.000     0.998     0.999      4713
           1      0.997     0.999     0.998      5353
           2      0.997     0.999     0.998      4828
           3      0.999     0.997     0.998      4902
           4      0.999     0.996     0.997      4654
           5      0.999     0.997     0.998      4344
           6      0.995     0.998     0.997      4716
           7      0.998     0.996     0.997      5036
           8      0.995     0.997     0.996      4723
           9      0.994     0.997     0.995      4731

    accuracy                          0.997     48000
   macro avg      0.997     0.997     0.997     48000
weighted avg      0.997     0.997     0.997     48000

[[ 973    0    0    0    0    1    2    1    3    0]
 [   0 1130    1    1    0    1    1    0    1    0]
 [   0    1 1026    0    0    0    0    2    3    0]
 [   0    0    0 1006    0    0    0    2    2    0]
 [   0    0    1    0  969    0    4    2    0    6]
 [   2    0    0   10    0  876    1    0    1    2]
 [   1    2    0    1    0    5  947    0    2    0]
 [   0    4    6    1    0    0    0 1008    1    8]
 [   1    0    1    2    0    1    0    2  965    2]
 [   1    2    1    1    3    3    0    4    2  992]]
              precision    recall  f1-score   support

           0      0.995     0.993     0.994       980
           1      0.992     0.996     0.994      1135
           2      0.990     0.994     0.992      1032
           3      0.984     0.996     0.990      1010
           4      0.997     0.987     0.992       982
           5      0.988     0.982     0.985       892
           6      0.992     0.989     0.990       958
           7      0.987     0.981     0.984      1028
           8      0.985     0.991     0.988       974
           9      0.982     0.983     0.983      1009

    accuracy                          0.989     10000
   macro avg      0.989     0.989     0.989     10000
weighted avg      0.989     0.989     0.989     10000

Epoch: 6 of 10, Train Acc: 99.731, Test Acc: 98.920, Loss: 0.019
[Epoch 7, 1200 / 12000 Mini Batches] loss: 0.009
[Epoch 7, 2400 / 12000 Mini Batches] loss: 0.010
[Epoch 7, 3600 / 12000 Mini Batches] loss: 0.013
[Epoch 7, 4800 / 12000 Mini Batches] loss: 0.012
[Epoch 7, 6000 / 12000 Mini Batches] loss: 0.012
[Epoch 7, 7200 / 12000 Mini Batches] loss: 0.013
[Epoch 7, 8400 / 12000 Mini Batches] loss: 0.014
[Epoch 7, 9600 / 12000 Mini Batches] loss: 0.015
[Epoch 7, 10800 / 12000 Mini Batches] loss: 0.015
[Epoch 7, 12000 / 12000 Mini Batches] loss: 0.014
[[4710    3    0    0    0    0    0    0    0    0]
 [   0 5345    1    0    1    0    0    5    1    0]
 [   0    2 4824    0    0    0    0    2    0    0]
 [   0    0    0 4900    0    0    0    1    0    1]
 [   0    0    0    0 4649    0    0    0    0    5]
 [   3    0    0   40    3 4283    1    0    6    8]
 [   4    3    1    0   10    0 4697    0    1    0]
 [   2    4    3    1    0    0    0 5026    0    0]
 [   1    3    2    9    4    0    0    1 4697    6]
 [   1    0    0    1    2    0    0    6    0 4721]]
              precision    recall  f1-score   support

           0      0.998     0.999     0.999      4713
           1      0.997     0.999     0.998      5353
           2      0.999     0.999     0.999      4828
           3      0.990     1.000     0.995      4902
           4      0.996     0.999     0.997      4654
           5      1.000     0.986     0.993      4344
           6      1.000     0.996     0.998      4716
           7      0.997     0.998     0.998      5036
           8      0.998     0.994     0.996      4723
           9      0.996     0.998     0.997      4731

    accuracy                          0.997     48000
   macro avg      0.997     0.997     0.997     48000
weighted avg      0.997     0.997     0.997     48000

[[ 977    0    0    0    0    0    0    0    2    1]
 [   0 1134    1    0    0    0    0    0    0    0]
 [   0    2 1025    1    1    0    0    2    1    0]
 [   0    0    0 1009    0    0    0    1    0    0]
 [   0    0    1    0  976    0    0    1    0    4]
 [   2    0    0   17    0  866    1    1    2    3]
 [   5    3    0    1    6    3  938    0    2    0]
 [   0    2    3    1    0    0    0 1014    1    7]
 [   2    0    2    7    1    1    0    2  954    5]
 [   0    2    0    3    5    2    0    3    1  993]]
              precision    recall  f1-score   support

           0      0.991     0.997     0.994       980
           1      0.992     0.999     0.996      1135
           2      0.993     0.993     0.993      1032
           3      0.971     0.999     0.985      1010
           4      0.987     0.994     0.990       982
           5      0.993     0.971     0.982       892
           6      0.999     0.979     0.989       958
           7      0.990     0.986     0.988      1028
           8      0.991     0.979     0.985       974
           9      0.980     0.984     0.982      1009

    accuracy                          0.989     10000
   macro avg      0.989     0.988     0.988     10000
weighted avg      0.989     0.989     0.989     10000

Epoch: 7 of 10, Train Acc: 99.692, Test Acc: 98.860, Loss: 0.014
[Epoch 8, 1200 / 12000 Mini Batches] loss: 0.007
[Epoch 8, 2400 / 12000 Mini Batches] loss: 0.008
[Epoch 8, 3600 / 12000 Mini Batches] loss: 0.010
[Epoch 8, 4800 / 12000 Mini Batches] loss: 0.009
[Epoch 8, 6000 / 12000 Mini Batches] loss: 0.010
[Epoch 8, 7200 / 12000 Mini Batches] loss: 0.010
[Epoch 8, 8400 / 12000 Mini Batches] loss: 0.010
[Epoch 8, 9600 / 12000 Mini Batches] loss: 0.011
[Epoch 8, 10800 / 12000 Mini Batches] loss: 0.011
[Epoch 8, 12000 / 12000 Mini Batches] loss: 0.011
[[4688    0    3    0    0   20    0    0    2    0]
 [   0 5331    4    0    0    1    1    8    6    2]
 [   0    0 4826    0    0    0    0    2    0    0]
 [   0    0    3 4891    0    6    0    1    0    1]
 [   0    0    0    0 4643    1    0    0    0   10]
 [   0    0    0    1    0 4342    0    0    1    0]
 [  25    0    3    0    0   37 4646    0    5    0]
 [   0    0    1    0    0    0    0 5034    1    0]
 [   1    0    9    3    3   16    0    1 4690    0]
 [   0    0    0    2    0   13    0    6    0 4710]]
              precision    recall  f1-score   support

           0      0.994     0.995     0.995      4713
           1      1.000     0.996     0.998      5353
           2      0.995     1.000     0.997      4828
           3      0.999     0.998     0.998      4902
           4      0.999     0.998     0.998      4654
           5      0.979     1.000     0.989      4344
           6      1.000     0.985     0.992      4716
           7      0.996     1.000     0.998      5036
           8      0.997     0.993     0.995      4723
           9      0.997     0.996     0.996      4731

    accuracy                          0.996     48000
   macro avg      0.996     0.996     0.996     48000
weighted avg      0.996     0.996     0.996     48000

[[ 974    0    1    0    0    2    0    1    2    0]
 [   1 1124    2    1    0    1    1    1    3    1]
 [   0    1 1029    0    0    0    0    2    0    0]
 [   0    0    2  995    0   10    0    3    0    0]
 [   0    0    1    0  972    0    1    2    0    6]
 [   0    0    0    4    0  887    1    0    0    0]
 [   9    2    0    1    2   12  929    0    3    0]
 [   0    0    7    0    0    0    0 1019    1    1]
 [   1    0    3    8    0    5    0    1  954    2]
 [   0    2    0    0    5    7    0    3    1  991]]
              precision    recall  f1-score   support

           0      0.989     0.994     0.991       980
           1      0.996     0.990     0.993      1135
           2      0.985     0.997     0.991      1032
           3      0.986     0.985     0.986      1010
           4      0.993     0.990     0.991       982
           5      0.960     0.994     0.977       892
           6      0.997     0.970     0.983       958
           7      0.987     0.991     0.989      1028
           8      0.990     0.979     0.985       974
           9      0.990     0.982     0.986      1009

    accuracy                          0.987     10000
   macro avg      0.987     0.987     0.987     10000
weighted avg      0.988     0.987     0.987     10000

Epoch: 8 of 10, Train Acc: 99.585, Test Acc: 98.740, Loss: 0.011
[Epoch 9, 1200 / 12000 Mini Batches] loss: 0.005
[Epoch 9, 2400 / 12000 Mini Batches] loss: 0.006
[Epoch 9, 3600 / 12000 Mini Batches] loss: 0.007
[Epoch 9, 4800 / 12000 Mini Batches] loss: 0.008
[Epoch 9, 6000 / 12000 Mini Batches] loss: 0.008
[Epoch 9, 7200 / 12000 Mini Batches] loss: 0.008
[Epoch 9, 8400 / 12000 Mini Batches] loss: 0.009
[Epoch 9, 9600 / 12000 Mini Batches] loss: 0.009
[Epoch 9, 10800 / 12000 Mini Batches] loss: 0.008
[Epoch 9, 12000 / 12000 Mini Batches] loss: 0.009
[[4708    0    2    1    0    0    2    0    0    0]
 [   0 5304   30    1    0    0    0   11    7    0]
 [   0    0 4825    1    0    0    2    0    0    0]
 [   0    0    0 4902    0    0    0    0    0    0]
 [   0    0    1    0 4643    0    1    0    0    9]
 [   0    0    0    8    0 4334    0    0    2    0]
 [   0    0    0    0    0    0 4715    0    1    0]
 [   0    1    3    0    0    0    0 5032    0    0]
 [   0    0    4   10    1    0    1    0 4707    0]
 [   0    0    0   27    0    4    0    7    0 4693]]
              precision    recall  f1-score   support

           0      1.000     0.999     0.999      4713
           1      1.000     0.991     0.995      5353
           2      0.992     0.999     0.996      4828
           3      0.990     1.000     0.995      4902
           4      1.000     0.998     0.999      4654
           5      0.999     0.998     0.998      4344
           6      0.999     1.000     0.999      4716
           7      0.996     0.999     0.998      5036
           8      0.998     0.997     0.997      4723
           9      0.998     0.992     0.995      4731

    accuracy                          0.997     48000
   macro avg      0.997     0.997     0.997     48000
weighted avg      0.997     0.997     0.997     48000

[[ 973    0    0    0    0    0    4    1    2    0]
 [   1 1119    6    2    0    2    1    1    3    0]
 [   0    0 1030    1    0    0    0    1    0    0]
 [   0    0    0 1009    0    0    0    1    0    0]
 [   0    0    1    0  971    0    1    2    0    7]
 [   1    0    0   13    0  877    1    0    0    0]
 [   2    2    0    1    2    4  947    0    0    0]
 [   0    0    7    0    0    0    0 1020    1    0]
 [   1    0    2    8    0    2    0    1  958    2]
 [   0    2    2    4    2    4    1    4    1  989]]
              precision    recall  f1-score   support

           0      0.995     0.993     0.994       980
           1      0.996     0.986     0.991      1135
           2      0.983     0.998     0.990      1032
           3      0.972     0.999     0.985      1010
           4      0.996     0.989     0.992       982
           5      0.987     0.983     0.985       892
           6      0.992     0.989     0.990       958
           7      0.989     0.992     0.991      1028
           8      0.993     0.984     0.988       974
           9      0.991     0.980     0.986      1009

    accuracy                          0.989     10000
   macro avg      0.989     0.989     0.989     10000
weighted avg      0.989     0.989     0.989     10000

Epoch: 9 of 10, Train Acc: 99.715, Test Acc: 98.930, Loss: 0.009
[Epoch 10, 1200 / 12000 Mini Batches] loss: 0.007
[Epoch 10, 2400 / 12000 Mini Batches] loss: 0.005
[Epoch 10, 3600 / 12000 Mini Batches] loss: 0.006
[Epoch 10, 4800 / 12000 Mini Batches] loss: 0.006
[Epoch 10, 6000 / 12000 Mini Batches] loss: 0.006
[Epoch 10, 7200 / 12000 Mini Batches] loss: 0.006
[Epoch 10, 8400 / 12000 Mini Batches] loss: 0.007
[Epoch 10, 9600 / 12000 Mini Batches] loss: 0.007
[Epoch 10, 10800 / 12000 Mini Batches] loss: 0.007
[Epoch 10, 12000 / 12000 Mini Batches] loss: 0.007
[[4713    0    0    0    0    0    0    0    0    0]
 [   0 5344    7    0    0    0    1    1    0    0]
 [   0    0 4828    0    0    0    0    0    0    0]
 [   0    0    1 4895    0    3    0    1    1    1]
 [   0    0    0    0 4642    0    0    0    0   12]
 [   1    0    0    2    0 4340    1    0    0    0]
 [   0    0    0    0    0    2 4714    0    0    0]
 [   0    1    0    0    0    0    0 5033    0    2]
 [   0    2    0    0    0    2    0    0 4719    0]
 [   0    0    0    0    0    0    0    1    0 4730]]
              precision    recall  f1-score   support

           0      1.000     1.000     1.000      4713
           1      0.999     0.998     0.999      5353
           2      0.998     1.000     0.999      4828
           3      1.000     0.999     0.999      4902
           4      1.000     0.997     0.999      4654
           5      0.998     0.999     0.999      4344
           6      1.000     1.000     1.000      4716
           7      0.999     0.999     0.999      5036
           8      1.000     0.999     0.999      4723
           9      0.997     1.000     0.998      4731

    accuracy                          0.999     48000
   macro avg      0.999     0.999     0.999     48000
weighted avg      0.999     0.999     0.999     48000

[[ 977    0    0    0    0    0    1    0    1    1]
 [   0 1130    2    0    0    1    1    1    0    0]
 [   0    0 1031    0    0    0    0    1    0    0]
 [   0    0    2  998    0    5    0    0    3    2]
 [   0    0    1    0  968    0    3    2    1    7]
 [   2    0    0    6    0  882    1    0    1    0]
 [   3    2    0    1    1    7  943    0    1    0]
 [   0    3    8    0    0    0    0 1013    1    3]
 [   3    0    2    2    0    3    0    1  959    4]
 [   0    2    0    0    3    5    0    4    0  995]]
              precision    recall  f1-score   support

           0      0.992     0.997     0.994       980
           1      0.994     0.996     0.995      1135
           2      0.986     0.999     0.992      1032
           3      0.991     0.988     0.990      1010
           4      0.996     0.986     0.991       982
           5      0.977     0.989     0.983       892
           6      0.994     0.984     0.989       958
           7      0.991     0.985     0.988      1028
           8      0.992     0.985     0.988       974
           9      0.983     0.986     0.985      1009

    accuracy                          0.990     10000
   macro avg      0.989     0.989     0.989     10000
weighted avg      0.990     0.990     0.990     10000

Epoch: 10 of 10, Train Acc: 99.912, Test Acc: 98.960, Loss: 0.007
300.67 minutes

Process finished with exit code 0
