C:\Users\stong\Anaconda3\python.exe C:/Users/stong/PycharmProjects/Project_2020_Introvert/main.py

1: LeNet
2: AlexNet
3: VGG-16
4: LeNet_improved

Enter 1-4 to select a model:
1

----Dataset Ratios----
Total dataset size: 70000
Train Set: 68.57%
Validation Set: 17.14%
Test Set: 14.29%

torch.Size([150, 1, 32, 32])
tensor([5, 6, 7, 2, 5, 1, 7, 6, 1, 1, 1, 3, 8, 3, 6, 6, 6, 3, 9, 4, 6, 9, 9, 8,
        2, 3, 1, 3, 2, 4, 4, 9, 1, 2, 0, 0, 9, 6, 0, 8, 6, 7, 8, 2, 5, 4, 6, 6,
        6, 1, 9, 5, 2, 8, 1, 7, 8, 6, 4, 9, 6, 4, 8, 1, 5, 7, 9, 3, 9, 9, 9, 6,
        2, 8, 7, 7, 0, 8, 3, 6, 1, 3, 0, 1, 5, 7, 3, 5, 2, 4, 7, 6, 3, 1, 6, 6,
        4, 6, 1, 7, 1, 1, 6, 3, 9, 5, 8, 5, 7, 0, 3, 8, 3, 5, 8, 0, 5, 3, 7, 6,
        9, 9, 2, 8, 2, 4, 0, 4, 6, 6, 6, 3, 5, 2, 0, 5, 2, 7, 5, 0, 4, 2, 3, 0,
        7, 1, 0, 8, 4, 0])
LeNet(
  (features): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): Tanh()
    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (4): Tanh()
    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): Tanh()
    (2): Linear(in_features=120, out_features=84, bias=True)
    (3): Tanh()
    (4): Linear(in_features=84, out_features=10, bias=True)
  )
)

LeNet activated!

cuda:0
[Epoch 1, 32 / 320 Mini Batches] loss: 2.247
[Epoch 1, 64 / 320 Mini Batches] loss: 2.170
[Epoch 1, 96 / 320 Mini Batches] loss: 2.062
[Epoch 1, 128 / 320 Mini Batches] loss: 1.930
[Epoch 1, 160 / 320 Mini Batches] loss: 1.801
[Epoch 1, 192 / 320 Mini Batches] loss: 1.684
[Epoch 1, 224 / 320 Mini Batches] loss: 1.577
[Epoch 1, 256 / 320 Mini Batches] loss: 1.486
[Epoch 1, 288 / 320 Mini Batches] loss: 1.406
[Epoch 1, 320 / 320 Mini Batches] loss: 1.334
[[4602    5   19    8   15   26   45   20   27    2]
 [   1 5241   38   12    4   30    6   15   91    9]
 [  61  107 3908   72  148   21  192  102  101    1]
 [  46  128  155 4039    6  206   23  100  168   37]
 [   7   30    9    0 4010    3  165    8   37  368]
 [ 232  130   56  210   86 3197  131   39  200   59]
 [  58   64   74    1   65   96 4367    0    7    0]
 [  34  171   60    4   48    3    3 4467   15  203]
 [  34  335   74  135   86  164   58   36 3692   73]
 [  62   82   18   58  711   34   26  387   80 3301]]
              precision    recall  f1-score   support

           0      0.896     0.965     0.929      4769
           1      0.833     0.962     0.893      5447
           2      0.886     0.829     0.857      4713
           3      0.890     0.823     0.855      4908
           4      0.774     0.865     0.817      4637
           5      0.846     0.737     0.787      4340
           6      0.871     0.923     0.896      4732
           7      0.863     0.892     0.877      5008
           8      0.836     0.788     0.811      4687
           9      0.814     0.694     0.749      4759

    accuracy                          0.851     48000
   macro avg      0.851     0.848     0.847     48000
weighted avg      0.851     0.851     0.849     48000

[[ 963    0    2    1    0    5    6    1    2    0]
 [   0 1100    4    3    0    3    3    0   22    0]
 [  16   31  848   11   31    5   33   25   31    1]
 [   6   18   33  859    0   37    2   24   25    6]
 [   1    8    2    0  868    1   30    1    6   65]
 [  41   21    9   50   22  669   26   14   25   15]
 [  20    7   11    1   14   24  878    1    2    0]
 [   5   41   30    1   10    0    0  905    4   32]
 [  13   38   10   27   22   41   11   18  778   16]
 [  15   12    1    6  172   12    5   55   19  712]]
              precision    recall  f1-score   support

           0      0.892     0.983     0.935       980
           1      0.862     0.969     0.912      1135
           2      0.893     0.822     0.856      1032
           3      0.896     0.850     0.873      1010
           4      0.762     0.884     0.818       982
           5      0.839     0.750     0.792       892
           6      0.883     0.916     0.900       958
           7      0.867     0.880     0.874      1028
           8      0.851     0.799     0.824       974
           9      0.841     0.706     0.767      1009

    accuracy                          0.858     10000
   macro avg      0.859     0.856     0.855     10000
weighted avg      0.859     0.858     0.856     10000

Epoch: 1 of 10, Train Acc: 85.050, Test Acc: 85.800, Loss: 1.334
[Epoch 2, 32 / 320 Mini Batches] loss: 0.649
[Epoch 2, 64 / 320 Mini Batches] loss: 0.616
[Epoch 2, 96 / 320 Mini Batches] loss: 0.604
[Epoch 2, 128 / 320 Mini Batches] loss: 0.590
[Epoch 2, 160 / 320 Mini Batches] loss: 0.575
[Epoch 2, 192 / 320 Mini Batches] loss: 0.563
[Epoch 2, 224 / 320 Mini Batches] loss: 0.552
[Epoch 2, 256 / 320 Mini Batches] loss: 0.539
[Epoch 2, 288 / 320 Mini Batches] loss: 0.529
[Epoch 2, 320 / 320 Mini Batches] loss: 0.519
[[4593    3   19    8   14   34   53   18   23    4]
 [   0 5261   42   17    9   34    7   15   53    9]
 [  45   48 4014   82  144   17  146   93  118    6]
 [  29   51  130 4287    2  167   30   83   89   40]
 [   4   21   19    1 4173    7   94    5   23  290]
 [ 109   70   39  199   59 3545  130   34  120   35]
 [  38   25   45    0   68   75 4474    0    7    0]
 [  20   87   73   16   45   12    1 4534   14  206]
 [  20  158   60  176   43  163   51   27 3920   69]
 [  46   33   22   64  280   42   11  251   47 3963]]
              precision    recall  f1-score   support

           0      0.937     0.963     0.950      4769
           1      0.914     0.966     0.939      5447
           2      0.899     0.852     0.875      4713
           3      0.884     0.873     0.879      4908
           4      0.863     0.900     0.881      4637
           5      0.865     0.817     0.840      4340
           6      0.895     0.945     0.920      4732
           7      0.896     0.905     0.901      5008
           8      0.888     0.836     0.861      4687
           9      0.857     0.833     0.845      4759

    accuracy                          0.891     48000
   macro avg      0.890     0.889     0.889     48000
weighted avg      0.891     0.891     0.890     48000

[[ 957    0    1    2    0    8    9    1    2    0]
 [   0 1105    4    4    0    3    3    1   15    0]
 [  12   10  897   12   22    3   24   18   31    3]
 [   4    4   21  914    1   24    3   19   14    6]
 [   1    5    3    0  891    1   18    0    5   58]
 [  19    5    8   49   12  727   23   12   27   10]
 [  12    5    7    1   14   16  900    1    2    0]
 [   5   22   31    5    6    0    0  912    4   43]
 [   5   14    9   32   15   38   13   13  822   13]
 [  10    7    3    6   69   15    2   38   13  846]]
              precision    recall  f1-score   support

           0      0.934     0.977     0.955       980
           1      0.939     0.974     0.956      1135
           2      0.912     0.869     0.890      1032
           3      0.892     0.905     0.898      1010
           4      0.865     0.907     0.886       982
           5      0.871     0.815     0.842       892
           6      0.905     0.939     0.922       958
           7      0.899     0.887     0.893      1028
           8      0.879     0.844     0.861       974
           9      0.864     0.838     0.851      1009

    accuracy                          0.897     10000
   macro avg      0.896     0.896     0.895     10000
weighted avg      0.897     0.897     0.897     10000

Epoch: 2 of 10, Train Acc: 89.092, Test Acc: 89.710, Loss: 0.519
[Epoch 3, 32 / 320 Mini Batches] loss: 0.406
[Epoch 3, 64 / 320 Mini Batches] loss: 0.403
[Epoch 3, 96 / 320 Mini Batches] loss: 0.400
[Epoch 3, 128 / 320 Mini Batches] loss: 0.394
[Epoch 3, 160 / 320 Mini Batches] loss: 0.390
[Epoch 3, 192 / 320 Mini Batches] loss: 0.387
[Epoch 3, 224 / 320 Mini Batches] loss: 0.384
[Epoch 3, 256 / 320 Mini Batches] loss: 0.380
[Epoch 3, 288 / 320 Mini Batches] loss: 0.377
[Epoch 3, 320 / 320 Mini Batches] loss: 0.373
[[4634    2   17    4   12   27   33   10   26    4]
 [   0 5258   42   15    9   31    4   17   60   11]
 [  46   34 4145   56  104   15  103   73  128    9]
 [  24   39  118 4244    3  205   26   81  125   43]
 [   4   16   32    1 4244    6   69    6   19  240]
 [  91   41   39  120   50 3717   85   21  141   35]
 [  38   14   38    0   58   74 4497    0   13    0]
 [  22   42   75   15   53   10    0 4619   14  158]
 [  25   90   41   96   31  117   30   13 4178   66]
 [  41   24   26   48  194   46    6  192   48 4134]]
              precision    recall  f1-score   support

           0      0.941     0.972     0.956      4769
           1      0.946     0.965     0.955      5447
           2      0.906     0.879     0.893      4713
           3      0.923     0.865     0.893      4908
           4      0.892     0.915     0.903      4637
           5      0.875     0.856     0.866      4340
           6      0.927     0.950     0.938      4732
           7      0.918     0.922     0.920      5008
           8      0.879     0.891     0.885      4687
           9      0.880     0.869     0.874      4759

    accuracy                          0.910     48000
   macro avg      0.909     0.909     0.908     48000
weighted avg      0.910     0.910     0.910     48000

[[ 963    0    1    1    0    6    6    1    2    0]
 [   0 1105    3    3    0    2    3    1   18    0]
 [  14    6  917    7   17    1   19   17   30    4]
 [   4    3   23  903    0   28    2   16   24    7]
 [   1    4    3    0  908    1   13    0    7   45]
 [  15    4   10   29    8  761   14    8   35    8]
 [  11    3    5    1   12   18  906    0    2    0]
 [   6   11   33    3    7    0    0  925    3   40]
 [   6    5    5   20   10   23    7   12  874   12]
 [  12    5    3    5   45   11    1   25   15  887]]
              precision    recall  f1-score   support

           0      0.933     0.983     0.957       980
           1      0.964     0.974     0.969      1135
           2      0.914     0.889     0.901      1032
           3      0.929     0.894     0.911      1010
           4      0.902     0.925     0.913       982
           5      0.894     0.853     0.873       892
           6      0.933     0.946     0.939       958
           7      0.920     0.900     0.910      1028
           8      0.865     0.897     0.881       974
           9      0.884     0.879     0.882      1009

    accuracy                          0.915     10000
   macro avg      0.914     0.914     0.914     10000
weighted avg      0.915     0.915     0.915     10000

Epoch: 3 of 10, Train Acc: 90.979, Test Acc: 91.490, Loss: 0.373
[Epoch 4, 32 / 320 Mini Batches] loss: 0.318
[Epoch 4, 64 / 320 Mini Batches] loss: 0.324
[Epoch 4, 96 / 320 Mini Batches] loss: 0.324
[Epoch 4, 128 / 320 Mini Batches] loss: 0.317
[Epoch 4, 160 / 320 Mini Batches] loss: 0.314
[Epoch 4, 192 / 320 Mini Batches] loss: 0.312
[Epoch 4, 224 / 320 Mini Batches] loss: 0.309
[Epoch 4, 256 / 320 Mini Batches] loss: 0.306
[Epoch 4, 288 / 320 Mini Batches] loss: 0.306
[Epoch 4, 320 / 320 Mini Batches] loss: 0.303
[[4634    2   14    7   11   24   39    9   26    3]
 [   0 5277   44   17    9   27    2   14   44   13]
 [  36   21 4227   52   92   14   88   70  103   10]
 [  19   28  103 4394    1  148   24   64   93   34]
 [   2   16   32    1 4322    3   53    7   14  187]
 [  52   38   27  128   36 3821   88   13  104   33]
 [  28   14   15    0   46   59 4561    0    8    1]
 [  19   35   74   21   47   10    0 4674   12  116]
 [  19   74   32  109   25   98   41    9 4210   70]
 [  29   24   22   56  190   37    6  165   41 4189]]
              precision    recall  f1-score   support

           0      0.958     0.972     0.965      4769
           1      0.954     0.969     0.962      5447
           2      0.921     0.897     0.909      4713
           3      0.918     0.895     0.907      4908
           4      0.904     0.932     0.918      4637
           5      0.901     0.880     0.891      4340
           6      0.930     0.964     0.947      4732
           7      0.930     0.933     0.932      5008
           8      0.904     0.898     0.901      4687
           9      0.900     0.880     0.890      4759

    accuracy                          0.923     48000
   macro avg      0.922     0.922     0.922     48000
weighted avg      0.923     0.923     0.923     48000

[[ 960    0    3    1    0    5    8    1    2    0]
 [   0 1113    4    3    0    1    3    1   10    0]
 [  10    6  931    7   14    1   16   15   28    4]
 [   3    2   20  934    0   19    2   12   14    4]
 [   1    4    4    1  923    1   11    0    3   34]
 [   8    2    6   35    8  781   17    5   24    6]
 [   8    3    5    1   11   12  916    0    2    0]
 [   6   10   31    8    7    0    0  938    1   27]
 [   4    4    7   18    9   17    8   13  885    9]
 [  12    6    3   10   47   10    2   22    8  889]]
              precision    recall  f1-score   support

           0      0.949     0.980     0.964       980
           1      0.968     0.981     0.974      1135
           2      0.918     0.902     0.910      1032
           3      0.917     0.925     0.921      1010
           4      0.906     0.940     0.923       982
           5      0.922     0.876     0.898       892
           6      0.932     0.956     0.944       958
           7      0.931     0.912     0.922      1028
           8      0.906     0.909     0.907       974
           9      0.914     0.881     0.897      1009

    accuracy                          0.927     10000
   macro avg      0.926     0.926     0.926     10000
weighted avg      0.927     0.927     0.927     10000

Epoch: 4 of 10, Train Acc: 92.310, Test Acc: 92.700, Loss: 0.303
[Epoch 5, 32 / 320 Mini Batches] loss: 0.272
[Epoch 5, 64 / 320 Mini Batches] loss: 0.271
[Epoch 5, 96 / 320 Mini Batches] loss: 0.269
[Epoch 5, 128 / 320 Mini Batches] loss: 0.268
[Epoch 5, 160 / 320 Mini Batches] loss: 0.268
[Epoch 5, 192 / 320 Mini Batches] loss: 0.266
[Epoch 5, 224 / 320 Mini Batches] loss: 0.264
[Epoch 5, 256 / 320 Mini Batches] loss: 0.262
[Epoch 5, 288 / 320 Mini Batches] loss: 0.257
[Epoch 5, 320 / 320 Mini Batches] loss: 0.256
[[4641    1   15    6    9   22   33    8   31    3]
 [   0 5293   43   18    9   25    2   11   33   13]
 [  27   16 4344   41   76   17   54   55   73   10]
 [  16   20   98 4428    2  143   19   55   92   35]
 [   2   15   30    1 4362    2   38    4   18  165]
 [  38   28   23  107   26 3931   68   10   79   30]
 [  26   10   15    0   40   61 4571    0    8    1]
 [  16   33   76   26   46   10    0 4654   14  133]
 [  16   59   35   95   19   87   30    5 4288   53]
 [  25   22   18   48  163   31    4  113   48 4287]]
              precision    recall  f1-score   support

           0      0.965     0.973     0.969      4769
           1      0.963     0.972     0.967      5447
           2      0.925     0.922     0.923      4713
           3      0.928     0.902     0.915      4908
           4      0.918     0.941     0.929      4637
           5      0.908     0.906     0.907      4340
           6      0.949     0.966     0.957      4732
           7      0.947     0.929     0.938      5008
           8      0.915     0.915     0.915      4687
           9      0.906     0.901     0.904      4759

    accuracy                          0.933     48000
   macro avg      0.932     0.933     0.932     48000
weighted avg      0.933     0.933     0.933     48000

[[ 963    0    2    0    1    6    5    1    2    0]
 [   0 1115    4    3    0    2    3    1    7    0]
 [  10    3  949    7   13    1   15   13   19    2]
 [   2    1   16  941    0   18    2   10   14    6]
 [   1    3    3    0  928    0   11    0    3   33]
 [   6    2    5   30    4  802   13    5   18    7]
 [   8    3    4    1   11   11  918    0    2    0]
 [   5    8   32    8    6    0    0  935    2   32]
 [   4    3    7   20    6   18    5    7  899    5]
 [   8    8    3    8   32    9    1   10   11  919]]
              precision    recall  f1-score   support

           0      0.956     0.983     0.969       980
           1      0.973     0.982     0.978      1135
           2      0.926     0.920     0.923      1032
           3      0.924     0.932     0.928      1010
           4      0.927     0.945     0.936       982
           5      0.925     0.899     0.912       892
           6      0.943     0.958     0.951       958
           7      0.952     0.910     0.930      1028
           8      0.920     0.923     0.922       974
           9      0.915     0.911     0.913      1009

    accuracy                          0.937     10000
   macro avg      0.936     0.936     0.936     10000
weighted avg      0.937     0.937     0.937     10000

Epoch: 5 of 10, Train Acc: 93.331, Test Acc: 93.690, Loss: 0.256
[Epoch 6, 32 / 320 Mini Batches] loss: 0.238
[Epoch 6, 64 / 320 Mini Batches] loss: 0.229
[Epoch 6, 96 / 320 Mini Batches] loss: 0.233
[Epoch 6, 128 / 320 Mini Batches] loss: 0.228
[Epoch 6, 160 / 320 Mini Batches] loss: 0.224
[Epoch 6, 192 / 320 Mini Batches] loss: 0.225
[Epoch 6, 224 / 320 Mini Batches] loss: 0.224
[Epoch 6, 256 / 320 Mini Batches] loss: 0.224
[Epoch 6, 288 / 320 Mini Batches] loss: 0.221
[Epoch 6, 320 / 320 Mini Batches] loss: 0.219
[[4640    1   14    5   10   17   42    9   28    3]
 [   0 5306   38   18    9   19    2   11   30   14]
 [  25   15 4391   36   64   12   44   53   66    7]
 [  11   17   90 4456    1  116   20   64   93   40]
 [   1   14   24    1 4385    2   40    3   16  151]
 [  28   22   13   91   23 3988   65    8   68   34]
 [  25   10   11    0   30   49 4598    0    8    1]
 [  14   26   59   17   38    8    0 4743   11   92]
 [  14   48   34   67   16   57   32    7 4369   43]
 [  21   20   16   43  143   25    4  113   52 4322]]
              precision    recall  f1-score   support

           0      0.971     0.973     0.972      4769
           1      0.968     0.974     0.971      5447
           2      0.936     0.932     0.934      4713
           3      0.941     0.908     0.924      4908
           4      0.929     0.946     0.937      4637
           5      0.929     0.919     0.924      4340
           6      0.949     0.972     0.960      4732
           7      0.947     0.947     0.947      5008
           8      0.922     0.932     0.927      4687
           9      0.918     0.908     0.913      4759

    accuracy                          0.942     48000
   macro avg      0.941     0.941     0.941     48000
weighted avg      0.942     0.942     0.942     48000

[[ 962    0    2    0    0    2   10    1    3    0]
 [   0 1115    3    3    0    1    3    1    9    0]
 [   8    2  958    6   13    2   11   11   20    1]
 [   0    1   13  947    0   14    2   13   15    5]
 [   1    1    4    0  931    0   11    1    4   29]
 [   7    1    2   29    4  811   13    3   16    6]
 [   8    3    3    0    7   10  925    0    2    0]
 [   5    8   25    4    4    0    0  956    2   24]
 [   3    2    6   15    5    8    4    8  918    5]
 [   6    8    1    8   28    9    2   10    9  928]]
              precision    recall  f1-score   support

           0      0.962     0.982     0.972       980
           1      0.977     0.982     0.980      1135
           2      0.942     0.928     0.935      1032
           3      0.936     0.938     0.937      1010
           4      0.939     0.948     0.943       982
           5      0.946     0.909     0.927       892
           6      0.943     0.966     0.954       958
           7      0.952     0.930     0.941      1028
           8      0.920     0.943     0.931       974
           9      0.930     0.920     0.925      1009

    accuracy                          0.945     10000
   macro avg      0.945     0.944     0.944     10000
weighted avg      0.945     0.945     0.945     10000

Epoch: 6 of 10, Train Acc: 94.162, Test Acc: 94.510, Loss: 0.219
[Epoch 7, 32 / 320 Mini Batches] loss: 0.196
[Epoch 7, 64 / 320 Mini Batches] loss: 0.195
[Epoch 7, 96 / 320 Mini Batches] loss: 0.193
[Epoch 7, 128 / 320 Mini Batches] loss: 0.193
[Epoch 7, 160 / 320 Mini Batches] loss: 0.190
[Epoch 7, 192 / 320 Mini Batches] loss: 0.191
[Epoch 7, 224 / 320 Mini Batches] loss: 0.192
[Epoch 7, 256 / 320 Mini Batches] loss: 0.191
[Epoch 7, 288 / 320 Mini Batches] loss: 0.191
[Epoch 7, 320 / 320 Mini Batches] loss: 0.190
[[4677    1   13    4    7   12   25    6   21    3]
 [   0 5331   36   15    9   16    0    8   18   14]
 [  25   15 4464   29   49    8   20   47   49    7]
 [  10   18   80 4532    3   96    8   61   65   35]
 [   2   14   26    1 4398    2   27    5   11  151]
 [  26   18   13   89   16 4044   48   10   47   29]
 [  28   10   13    0   27   49 4594    0   10    1]
 [  12   25   56   18   31    5    1 4789    6   65]
 [  13   53   33   75   15   52   23   10 4361   52]
 [  22   17   15   43   99   21    3  114   31 4394]]
              precision    recall  f1-score   support

           0      0.971     0.981     0.976      4769
           1      0.969     0.979     0.974      5447
           2      0.940     0.947     0.944      4713
           3      0.943     0.923     0.933      4908
           4      0.945     0.948     0.947      4637
           5      0.939     0.932     0.936      4340
           6      0.967     0.971     0.969      4732
           7      0.948     0.956     0.952      5008
           8      0.944     0.930     0.937      4687
           9      0.925     0.923     0.924      4759

    accuracy                          0.950     48000
   macro avg      0.949     0.949     0.949     48000
weighted avg      0.950     0.950     0.950     48000

[[ 968    0    2    0    0    2    5    1    2    0]
 [   0 1120    3    3    0    1    3    2    3    0]
 [   7    1  976    5   11    2    5    9   15    1]
 [   0    1   12  959    0   12    0   12    9    5]
 [   1    2    7    0  927    0    7    2    3   33]
 [   7    1    2   26    3  824   11    2   10    6]
 [   9    3    3    0    6   11  924    0    2    0]
 [   4    7   25    5    4    0    0  965    0   18]
 [   4    3    7   16    5    7    4    9  912    7]
 [   5    8    1   11   21    6    2   10    6  939]]
              precision    recall  f1-score   support

           0      0.963     0.988     0.975       980
           1      0.977     0.987     0.982      1135
           2      0.940     0.946     0.943      1032
           3      0.936     0.950     0.943      1010
           4      0.949     0.944     0.946       982
           5      0.953     0.924     0.938       892
           6      0.961     0.965     0.963       958
           7      0.954     0.939     0.946      1028
           8      0.948     0.936     0.942       974
           9      0.931     0.931     0.931      1009

    accuracy                          0.951     10000
   macro avg      0.951     0.951     0.951     10000
weighted avg      0.951     0.951     0.951     10000

Epoch: 7 of 10, Train Acc: 94.967, Test Acc: 95.140, Loss: 0.190
[Epoch 8, 32 / 320 Mini Batches] loss: 0.178
[Epoch 8, 64 / 320 Mini Batches] loss: 0.169
[Epoch 8, 96 / 320 Mini Batches] loss: 0.172
[Epoch 8, 128 / 320 Mini Batches] loss: 0.171
[Epoch 8, 160 / 320 Mini Batches] loss: 0.171
[Epoch 8, 192 / 320 Mini Batches] loss: 0.168
[Epoch 8, 224 / 320 Mini Batches] loss: 0.168
[Epoch 8, 256 / 320 Mini Batches] loss: 0.168
[Epoch 8, 288 / 320 Mini Batches] loss: 0.167
[Epoch 8, 320 / 320 Mini Batches] loss: 0.166
[[4687    1   10    3    7   11   22    4   21    3]
 [   0 5327   36   22    9    9    0    7   27   10]
 [  23   13 4509   26   40    8   12   34   42    6]
 [   6   13   73 4598    1   76    4   46   59   32]
 [   1   15   22    2 4382    2   27    5   11  170]
 [  15    9   12   84   11 4111   32   11   33   22]
 [  28    8    8    0   25   59 4595    0    8    1]
 [   9   24   67   23   24    4    0 4780    9   68]
 [   9   38   31   70   10   45   18    7 4416   43]
 [  18   16   11   45   75   23    1   91   31 4448]]
              precision    recall  f1-score   support

           0      0.977     0.983     0.980      4769
           1      0.975     0.978     0.976      5447
           2      0.944     0.957     0.950      4713
           3      0.944     0.937     0.940      4908
           4      0.956     0.945     0.950      4637
           5      0.945     0.947     0.946      4340
           6      0.975     0.971     0.973      4732
           7      0.959     0.954     0.957      5008
           8      0.948     0.942     0.945      4687
           9      0.926     0.935     0.930      4759

    accuracy                          0.955     48000
   macro avg      0.955     0.955     0.955     48000
weighted avg      0.955     0.955     0.955     48000

[[ 971    0    2    0    0    2    2    1    2    0]
 [   0 1118    2    3    0    3    2    1    6    0]
 [   6    1  985    6    8    1    3    8   13    1]
 [   0    1   11  969    0    8    0    9    7    5]
 [   1    3    7    0  925    0    6    2    2   36]
 [   6    1    1   22    1  841    7    1    7    5]
 [   9    3    2    0    6   12  923    0    3    0]
 [   4    7   27    6    3    0    0  966    0   15]
 [   3    1    6   14    4   10    4    8  921    3]
 [   5    9    0   11   19    6    2   10    6  941]]
              precision    recall  f1-score   support

           0      0.966     0.991     0.978       980
           1      0.977     0.985     0.981      1135
           2      0.944     0.954     0.949      1032
           3      0.940     0.959     0.950      1010
           4      0.958     0.942     0.950       982
           5      0.952     0.943     0.948       892
           6      0.973     0.963     0.968       958
           7      0.960     0.940     0.950      1028
           8      0.952     0.946     0.949       974
           9      0.935     0.933     0.934      1009

    accuracy                          0.956     10000
   macro avg      0.956     0.956     0.956     10000
weighted avg      0.956     0.956     0.956     10000

Epoch: 8 of 10, Train Acc: 95.527, Test Acc: 95.600, Loss: 0.166
[Epoch 9, 32 / 320 Mini Batches] loss: 0.164
[Epoch 9, 64 / 320 Mini Batches] loss: 0.152
[Epoch 9, 96 / 320 Mini Batches] loss: 0.153
[Epoch 9, 128 / 320 Mini Batches] loss: 0.150
[Epoch 9, 160 / 320 Mini Batches] loss: 0.151
[Epoch 9, 192 / 320 Mini Batches] loss: 0.149
[Epoch 9, 224 / 320 Mini Batches] loss: 0.148
[Epoch 9, 256 / 320 Mini Batches] loss: 0.148
[Epoch 9, 288 / 320 Mini Batches] loss: 0.148
[Epoch 9, 320 / 320 Mini Batches] loss: 0.147
[[4690    1    8    3    4    9   27    7   17    3]
 [   0 5345   36   21    9    3    1   10   15    7]
 [  12   13 4534   26   30    7   12   43   31    5]
 [   5   14   61 4646    1   60    5   54   36   26]
 [   1   16   20    2 4460    1   29    8    5   95]
 [  13    9    9   86   10 4121   40   12   23   17]
 [  24    6    5    0   21   42 4628    0    6    0]
 [   7   19   40   17   22    3    0 4852    4   44]
 [  10   42   30   83    9   40   28   10 4400   35]
 [  15   17    9   50  106   21    1  111   24 4405]]
              precision    recall  f1-score   support

           0      0.982     0.983     0.983      4769
           1      0.975     0.981     0.978      5447
           2      0.954     0.962     0.958      4713
           3      0.942     0.947     0.944      4908
           4      0.955     0.962     0.958      4637
           5      0.957     0.950     0.953      4340
           6      0.970     0.978     0.974      4732
           7      0.950     0.969     0.959      5008
           8      0.965     0.939     0.952      4687
           9      0.950     0.926     0.938      4759

    accuracy                          0.960     48000
   macro avg      0.960     0.960     0.960     48000
weighted avg      0.960     0.960     0.960     48000

[[ 969    0    2    0    0    2    4    1    2    0]
 [   0 1119    2    3    0    1    4    2    4    0]
 [   5    1  992    6    6    0    3    9    9    1]
 [   0    1    7  976    0    6    0   10    8    2]
 [   1    0    6    0  942    0    6    4    2   21]
 [   6    1    0   24    2  842    7    1    5    4]
 [   9    2    0    0    5    8  931    0    3    0]
 [   3    8   19    6    2    0    0  983    0    7]
 [   3    1    7   15    4   10    4   10  917    3]
 [   4    8    0   11   23    6    2   14    6  935]]
              precision    recall  f1-score   support

           0      0.969     0.989     0.979       980
           1      0.981     0.986     0.983      1135
           2      0.958     0.961     0.960      1032
           3      0.938     0.966     0.952      1010
           4      0.957     0.959     0.958       982
           5      0.962     0.944     0.953       892
           6      0.969     0.972     0.970       958
           7      0.951     0.956     0.953      1028
           8      0.959     0.941     0.950       974
           9      0.961     0.927     0.943      1009

    accuracy                          0.961     10000
   macro avg      0.960     0.960     0.960     10000
weighted avg      0.961     0.961     0.961     10000

Epoch: 9 of 10, Train Acc: 96.002, Test Acc: 96.060, Loss: 0.147
[Epoch 10, 32 / 320 Mini Batches] loss: 0.131
[Epoch 10, 64 / 320 Mini Batches] loss: 0.132
[Epoch 10, 96 / 320 Mini Batches] loss: 0.133
[Epoch 10, 128 / 320 Mini Batches] loss: 0.131
[Epoch 10, 160 / 320 Mini Batches] loss: 0.131
[Epoch 10, 192 / 320 Mini Batches] loss: 0.129
[Epoch 10, 224 / 320 Mini Batches] loss: 0.131
[Epoch 10, 256 / 320 Mini Batches] loss: 0.132
[Epoch 10, 288 / 320 Mini Batches] loss: 0.131
[Epoch 10, 320 / 320 Mini Batches] loss: 0.131
[[4696    1    5    2    4    7   24    4   22    4]
 [   0 5352   31   18    8    2    0    8   20    8]
 [  13   15 4551   21   26    5   10   36   31    5]
 [   6   13   56 4630    2   81    2   43   47   28]
 [   1   12   18    2 4499    2   21    4    5   73]
 [  11    6    6   46    7 4189   28    6   25   16]
 [  24    5    6    0   25   45 4618    0    9    0]
 [   6   23   42   18   23    4    0 4834    6   52]
 [   6   39   19   45   11   34   15    8 4485   25]
 [  14   14    6   34  115   22    1   87   27 4439]]
              precision    recall  f1-score   support

           0      0.983     0.985     0.984      4769
           1      0.977     0.983     0.980      5447
           2      0.960     0.966     0.963      4713
           3      0.961     0.943     0.952      4908
           4      0.953     0.970     0.962      4637
           5      0.954     0.965     0.960      4340
           6      0.979     0.976     0.977      4732
           7      0.961     0.965     0.963      5008
           8      0.959     0.957     0.958      4687
           9      0.955     0.933     0.944      4759

    accuracy                          0.964     48000
   macro avg      0.964     0.964     0.964     48000
weighted avg      0.964     0.964     0.964     48000

[[ 970    0    2    0    0    2    3    1    2    0]
 [   0 1123    2    3    0    1    3    0    3    0]
 [   5    1  993    6    3    0    3    9   11    1]
 [   0    1    8  970    0    9    0   10    8    4]
 [   1    1    6    0  952    0    5    2    2   13]
 [   4    1    0   14    2  857    5    1    5    3]
 [   9    3    1    0    5    9  928    0    3    0]
 [   3    8   20    5    3    0    0  980    1    8]
 [   4    1    4   10    4    7    4    7  931    2]
 [   3    9    0   11   28    7    1    8    7  935]]
              precision    recall  f1-score   support

           0      0.971     0.990     0.980       980
           1      0.978     0.989     0.984      1135
           2      0.958     0.962     0.960      1032
           3      0.952     0.960     0.956      1010
           4      0.955     0.969     0.962       982
           5      0.961     0.961     0.961       892
           6      0.975     0.969     0.972       958
           7      0.963     0.953     0.958      1028
           8      0.957     0.956     0.956       974
           9      0.968     0.927     0.947      1009

    accuracy                          0.964     10000
   macro avg      0.964     0.964     0.964     10000
weighted avg      0.964     0.964     0.964     10000

Epoch: 10 of 10, Train Acc: 96.444, Test Acc: 96.390, Loss: 0.131
6.34 minutes

Process finished with exit code 0
